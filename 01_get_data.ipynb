{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "# pip install lxml\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# debug\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph des directeurs de thèses --> thésards\n",
    "\n",
    "Avec les données du site theses.fr, et l'api:\n",
    "http://documentation.abes.fr/aidethesesfr/accueil/index.html#RecupererDonneesPagePersonne\n",
    "\n",
    "info sur les Parser XML de BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\n",
    "\n",
    "\n",
    "- Les info sur les thèses sont récupérées avec des requètes sur une personne.\n",
    "    - liste des thèses (la sienne, en tant que directeur, que rapporteur, et membre du jury)\n",
    "    \n",
    "\n",
    "puis sur les personnes connexes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_foaf(tag):\n",
    "    try:\n",
    "        name = tag.text\n",
    "        person_id = tag.findChild().attrs['rdf:about']\n",
    "    except KeyError:\n",
    "        # print(tag)\n",
    "        name, person_id = (tag.text, None)\n",
    "    return (name, person_id)\n",
    "\n",
    "\n",
    "def short_id(auth, year):\n",
    "    \"\"\" Robust id for people \n",
    "    \"\"\"\n",
    "    # th:   (('Name Name', 'http://www.theses.fr/0123456789/id'), 1296)\n",
    "    name = auth[0]\n",
    "    initials = ''.join( [u[0] for u in name.split()] )\n",
    "    idx = auth[1].split('/')[-2]\n",
    "    return initials+'_'+str(year)+'_'+idx\n",
    "\n",
    "requests_cache.install_cache('thesis_cache')\n",
    "\n",
    "def query_someone(person_id):\n",
    "    ''' Request info about the person\n",
    "        returns the person info (name and id) and list of related thesis\n",
    "    '''\n",
    "    url = person_id.replace('/id', '.xml')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'lxml') # Set the parser\n",
    "    \n",
    "    # Person info:\n",
    "    agent = soup.find('foaf:agent')\n",
    "    person_id = agent.attrs['rdf:about']\n",
    "    name = agent.find('foaf:name').text\n",
    "    person = (name, person_id)\n",
    "    \n",
    "    # Thesis list:\n",
    "    linked_thesis = []\n",
    "    for th in soup.find_all('bibo:thesis'):\n",
    "        info = dict()\n",
    "        info['title'] = th.find('dc:title').text\n",
    "        info['author'] = parse_foaf( th.find('marcrel:aut') )\n",
    "        info['id'] = th.attrs['rdf:about']\n",
    "        info['url'] = th.find('dc:identifier').text if th.find('dc:identifier') else ''\n",
    "        try:\n",
    "            info['year'] = int(th.find('dc:date').text)\n",
    "        except ValueError:\n",
    "            info['year'] = 0\n",
    "        info['directors'] = [parse_foaf(ths) for ths in th.find_all('marcrel:ths')]\n",
    "        info['doctoral_school'] = parse_foaf(th.find('dcterms:contributor')) if th.find('dcterms:contributor') else ()\n",
    "        info['univ'] = parse_foaf(th.find('marcrel:dgg'))\n",
    "        info['short_id'] = short_id(info['author'], info['year'])\n",
    "        \n",
    "        linked_thesis.append(info)\n",
    "\n",
    "    return person, linked_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_id(foaf_id):\n",
    "    return foaf_id.endswith('/id') and not foaf_id.endswith('//id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objet qui stocke les données explorées\n",
    "# et les requetes suivante à faire\n",
    "class Blob():\n",
    "    def __init__(self, seed_id):\n",
    "        self.already_asked = set()\n",
    "        self.to_search = set([seed_id, ])\n",
    "        self.thesis = dict()\n",
    "        self.seed = seed_id\n",
    "        self.chains = []\n",
    "        self.nbr_gen = 0\n",
    "        self.name = seed_id.split('/')[-2]\n",
    "        \n",
    "    def grow(self):\n",
    "        person_id = self.to_search.pop()\n",
    "       \n",
    "        full_id, results = query_someone(person_id)\n",
    "        print('\\r', full_id[0], '(%i theses)'%len(results), end=' '*20)\n",
    "        self.already_asked.add(person_id)\n",
    "        self.nbr_gen += 1\n",
    "        \n",
    "        # default value\n",
    "        own_thesis = {'author':full_id,\n",
    "                      'year':0,\n",
    "                      'id':None,\n",
    "                      'directors':[],\n",
    "                      'short_id':short_id(full_id, 0)} \n",
    "        \n",
    "        # Sort the thesis (own, as a director, other)\n",
    "        a_dirige = []\n",
    "        related = []     \n",
    "        for th in results:\n",
    "            if full_id == th['author']:\n",
    "                own_thesis = th\n",
    "            elif full_id in (d for d in th['directors']):\n",
    "                a_dirige.append(th)\n",
    "            else:\n",
    "                related.append(th)\n",
    "        \n",
    "        a_dirige.sort(key=lambda x:x['year'])\n",
    "\n",
    "        # Update new persons to search:\n",
    "        co_directors = {d for th in a_dirige+[own_thesis, ]\n",
    "                          for d in th['directors'] if d!=full_id}\n",
    "        students = {th['author'] for th in a_dirige}\n",
    "\n",
    "        new_person = [p[1] for p in co_directors | students\n",
    "                      if is_valid_id(p[1]) and p[1] not in self.already_asked]\n",
    "\n",
    "        self.to_search.update(new_person)\n",
    "        \n",
    "        # Save thesis:\n",
    "        self.thesis[own_thesis['short_id']] = own_thesis\n",
    "        self.thesis.update({th['short_id']:th for th in a_dirige})\n",
    "        \n",
    "        # Build 'chain' (for subway graph)\n",
    "        if len(a_dirige) > 0:\n",
    "            chain = [own_thesis['short_id'], ] + [th['short_id'] for th in a_dirige]\n",
    "            self.chains.append(chain)\n",
    "            \n",
    "        \n",
    "    def save(self, name='blob.p'):\n",
    "        pickle.dump(self, open( name, \"wb\" ))\n",
    "        print(\"saved\")\n",
    "        \n",
    "    def print_info(self):\n",
    "        print('nbr thesis:', len(blob.thesis))\n",
    "        print('nbr to search:', len(blob.to_search))\n",
    "        print(self.seed, '+', self.nbr_gen, 'generations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors\n",
    "tableau20 = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',\n",
    "             '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',\n",
    "             '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',\n",
    "             '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5']\n",
    "# http://vis.stanford.edu/color-names/analyzer/\n",
    "\n",
    "def get_color(string):\n",
    "    k = hash(string) % len(tableau20)\n",
    "    return tableau20[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seedlist loaded: 51 seeds\n",
      "http://www.theses.fr/162509553/id\n"
     ]
    }
   ],
   "source": [
    "# Get a random seed\n",
    "seedlist_file = \"data/seedlist.pick\"\n",
    "try:\n",
    "    seedlist = pickle.load( open( seedlist_file, \"rb\" ) )\n",
    "    print('seedlist loaded:', len(seedlist), \"seeds\")\n",
    "except FileNotFoundError:\n",
    "    seedlist = ['http://www.theses.fr/068670648/id', ]\n",
    "    pickle.dump(seedlist, open( seedlist_file, \"wb\" ))\n",
    "    seedlist = pickle.load( open( seedlist_file, \"rb\" ) )\n",
    "    print('seedlist loaded:', len(seedlist), \"seeds\")\n",
    "    \n",
    "seed_id = random.choice(seedlist)\n",
    "print(seed_id)\n",
    "\n",
    "# New graph\n",
    "blob = Blob(seed_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Cynthia Joseph Eid (2 theses)                    "
     ]
    }
   ],
   "source": [
    "blob.grow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bernard Bonnetot (3 theses)                                     \n",
      "\n",
      "nbr thesis: 129\n",
      "nbr to search: 140\n",
      "http://www.theses.fr/162509553/id + 41 generations\n"
     ]
    }
   ],
   "source": [
    "# Grow\n",
    "for _ in range(40):\n",
    "    blob.grow()\n",
    "\n",
    "print('\\n')\n",
    "blob.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/162509553_129\n",
      "19 chains\n"
     ]
    }
   ],
   "source": [
    "# Export the graph \n",
    "filename_prefix = 'data/'+blob.name+'_'+str(len(blob.thesis))\n",
    "print(filename_prefix)\n",
    "\n",
    "# Chains (for the layout)\n",
    "chs = sorted(blob.chains, key=len, reverse=True)\n",
    "print(len(chs), \"chains\")\n",
    "\n",
    "with open(filename_prefix + '_chains.json', 'w') as outfile:\n",
    "    json.dump(chs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 thesis\n"
     ]
    }
   ],
   "source": [
    "# Extract chain info and update\n",
    "chain_info = dict()\n",
    "for chain in blob.chains:\n",
    "    name = blob.thesis[chain[0]]['author'][0]\n",
    "    color = get_color(name)\n",
    "    chain_info[chain[0]] = {'length': len(chain),\n",
    "                            'color': color,\n",
    "                            'name': name,\n",
    "                            'year':blob.thesis[chain[0]]['year']}\n",
    "    for n in chain:\n",
    "        blob.thesis[n]['color'] = color\n",
    "        \n",
    "        if blob.thesis[n]['author'][1] in blob.already_asked:\n",
    "            blob.thesis[n]['searched'] = 1\n",
    "        else:\n",
    "            blob.thesis[n]['searched'] = 0\n",
    "            \n",
    "    blob.thesis[chain[0]]['nbr_thesis'] = len(chain)\n",
    "    #blob.thesis[chain[0]]['color'] = red\n",
    "    \n",
    "# Export the info\n",
    "with open(filename_prefix + '_chaininfo.json', 'w') as outfile:\n",
    "    json.dump(chain_info, outfile)\n",
    "\n",
    "# Export the thesis info\n",
    "with open(filename_prefix + '_thesis.json', 'w') as outfile:\n",
    "    json.dump(blob.thesis, outfile)\n",
    "    \n",
    "print(len(blob.thesis), 'thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "#blobname = 'data/grosblob6.p'\n",
    "#blob.save(blobname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_thesis(full_id, results):\n",
    "    # default value\n",
    "    own_thesis = {'author':full_id,\n",
    "                  'year':0,\n",
    "                  'id':None,\n",
    "                  'directors':[],\n",
    "                  'short_id':short_id(full_id, 0)} \n",
    "\n",
    "    # Sort the thesis (own, as a director, other)\n",
    "    a_dirige = []\n",
    "    related = []     \n",
    "    for th in results:\n",
    "        if full_id == th['author']:\n",
    "            own_thesis = th\n",
    "        elif full_id in (d for d in th['directors']):\n",
    "            a_dirige.append(th)\n",
    "        else:\n",
    "            related.append(th)\n",
    "    \n",
    "    a_dirige.sort(key=lambda x:x['year'])\n",
    "\n",
    "    return own_thesis, a_dirige, related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.theses.fr/068670648/id\n"
     ]
    }
   ],
   "source": [
    "seed_id = random.choice(seedlist)\n",
    "print(seed_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Arnaud Brioude', 'http://www.theses.fr/068670648/id') 25 11\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Arnaud Brioude', 'http://www.theses.fr/068670648/id') 25 11\n"
     ]
    }
   ],
   "source": [
    "# gen zero\n",
    "full_id, results = query_someone(seed_id)\n",
    "own_thesis, a_dirige, related = sort_thesis(full_id, results)\n",
    "\n",
    "print(full_id, len(results), len(a_dirige))\n",
    "\n",
    "thesis = dict()\n",
    "chains = []\n",
    "\n",
    "# save thesis\n",
    "thesis[own_thesis['short_id']] = own_thesis\n",
    "for th in a_dirige:\n",
    "    thesis[th['short_id']] = th\n",
    "\n",
    "# Build 'chain' (for subway graph)\n",
    "if len(a_dirige) > 0:\n",
    "    chain = [own_thesis['short_id'], ] + [th['short_id'] for th in a_dirige]\n",
    "    chains.append(chain)\n",
    "else:\n",
    "    print('no thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(thesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second gen 11\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "second_gen = {th['author'][1] for th in a_dirige if is_valid_id(th['author'][1])}\n",
    "print('second gen', len(second_gen))\n",
    "\n",
    "third_gen = set()\n",
    "for id_to_search in second_gen:\n",
    "    full_id, results = query_someone(id_to_search)\n",
    "    own_thesis, a_dirige, related = sort_thesis(full_id, results)\n",
    "    print(len(a_dirige))\n",
    "    # save thesis\n",
    "    thesis[own_thesis['short_id']] = own_thesis\n",
    "    for th in a_dirige:\n",
    "        thesis[th['short_id']] = th\n",
    "        \n",
    "    # Build 'chain' (for subway graph)\n",
    "    if len(a_dirige) > 0:\n",
    "        chain = [own_thesis['short_id'], ] + [th['short_id'] for th in a_dirige]\n",
    "        third_gen.update({th['author'][1] for th in a_dirige if is_valid_id(th['author'][1])})\n",
    "        chains.append(chain)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(thesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_third_gen_thesis = dict()\n",
    "for id_to_search in third_gen:\n",
    "    full_id, results = query_someone(id_to_search)\n",
    "    own_thesis, a_dirige, related = sort_thesis(full_id, results)\n",
    "    \n",
    "    th_already_known = len([th for th in a_dirige if th['short_id'] in thesis])\n",
    "    if own_thesis['short_id'] in thesis:\n",
    "        th_already_known += 1\n",
    "        \n",
    "    if th_already_known <= 1:\n",
    "        continue\n",
    "    else:\n",
    "        # save thesis\n",
    "        accepted_third_gen_thesis[own_thesis['short_id']] = own_thesis\n",
    "        for th in a_dirige:\n",
    "            accepted_third_gen_thesis[th['short_id']] = th\n",
    "\n",
    "        # Build 'chain' (for subway graph)\n",
    "        if len(a_dirige) > 0:\n",
    "            chain = [own_thesis['short_id'], ] + [th['short_id'] for th in a_dirige]\n",
    "            chains.append(chain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_third_gen_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_already_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-global",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
