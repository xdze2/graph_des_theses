{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# pip install lxml\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# debug\n",
    "import sys\n",
    "from pympler import asizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph des directeurs de thèses --> thésards\n",
    "\n",
    "Avec les données du site theses.fr, et l'api:\n",
    "http://documentation.abes.fr/aidethesesfr/accueil/index.html#RecupererDonneesPagePersonne\n",
    "\n",
    "info sur les Parser XML de BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\n",
    "\n",
    "\n",
    "- Les info sur les thèses sont récupérées avec des requètes sur une personne.\n",
    "    - liste des thèses (la sienne, en tant que directeur, que rapporteur, et membre du jury)\n",
    "    \n",
    "\n",
    "puis sur les personnes connexes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_foaf(tag):\n",
    "    try:\n",
    "        name = tag.text\n",
    "        person_id = tag.findChild().attrs['rdf:about']\n",
    "    except KeyError:\n",
    "        # print(tag)\n",
    "        name, person_id = (tag.text, None)\n",
    "    return (name, person_id)\n",
    "\n",
    "\n",
    "def short_id(auth, year):\n",
    "    \"\"\" Robust id for people \n",
    "    \"\"\"\n",
    "    # th:   (('Name Name', 'http://www.theses.fr/0123456789/id'), 1296)\n",
    "    name = auth[0]\n",
    "    initials = ''.join( [u[0] for u in name.split()] )\n",
    "    idx = auth[1].split('/')[-2]\n",
    "    return initials+'_'+str(year)+'_'+idx\n",
    "\n",
    "\n",
    "def query_someone(person_id):\n",
    "    ''' Request info about the person\n",
    "        returns the person info (name and id) and list of related thesis\n",
    "    '''\n",
    "    url = person_id.replace('/id', '.xml')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'lxml') # Set the parser\n",
    "    \n",
    "    # Person info:\n",
    "    agent = soup.find('foaf:agent')\n",
    "    person_id = agent.attrs['rdf:about']\n",
    "    name = agent.find('foaf:name').text\n",
    "    person = (name, person_id)\n",
    "    \n",
    "    # Thesis list:\n",
    "    linked_thesis = []\n",
    "    for th in soup.find_all('bibo:thesis'):\n",
    "        info = dict()\n",
    "        info['title'] = th.find('dc:title').text\n",
    "        info['author'] = parse_foaf( th.find('marcrel:aut') )\n",
    "        info['id'] = th.attrs['rdf:about']\n",
    "        info['url'] = th.find('dc:identifier').text if th.find('dc:identifier') else ''\n",
    "        try:\n",
    "            info['year'] = int(th.find('dc:date').text)\n",
    "        except ValueError:\n",
    "            info['year'] = 0\n",
    "        info['directors'] = [parse_foaf(ths) for ths in th.find_all('marcrel:ths')]\n",
    "        info['doctoral_school'] = parse_foaf(th.find('dcterms:contributor')) if th.find('dcterms:contributor') else ()\n",
    "        info['univ'] = parse_foaf(th.find('marcrel:dgg'))\n",
    "        info['short_id'] = short_id(info['author'], info['year'])\n",
    "        \n",
    "        linked_thesis.append(info)\n",
    "\n",
    "    return person, linked_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_id(foaf_id):\n",
    "    return foaf_id.endswith('/id') and not foaf_id.endswith('//id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objet qui stocke les données explorées\n",
    "# et les requetes suivante à faire\n",
    "class Blob():\n",
    "    def __init__(self, seed_id):\n",
    "        self.already_asked = set()\n",
    "        self.to_search = set([seed_id, ])\n",
    "        self.thesis = dict()\n",
    "        self.seed = seed_id\n",
    "        self.chains = []\n",
    "        self.nbr_gen = 0\n",
    "        self.name = seed_id.split('/')[-2]\n",
    "        \n",
    "    def grow(self):\n",
    "        person_id = self.to_search.pop()\n",
    "       \n",
    "        full_id, results = query_someone(person_id)\n",
    "        print('\\r', full_id[0], '(%i theses)'%len(results), end=' '*20)\n",
    "        self.already_asked.add(person_id)\n",
    "        self.nbr_gen += 1\n",
    "        \n",
    "        # default value\n",
    "        own_thesis = {'author':full_id,\n",
    "                      'year':0,\n",
    "                      'id':None,\n",
    "                      'directors':[],\n",
    "                      'short_id':short_id(full_id, 0)} \n",
    "        \n",
    "        # Sort the thesis (own, as a director, other)\n",
    "        a_dirige = []\n",
    "        related = []     \n",
    "        for th in results:\n",
    "            if full_id == th['author']:\n",
    "                own_thesis = th\n",
    "            elif full_id in (d for d in th['directors']):\n",
    "                a_dirige.append(th)\n",
    "            else:\n",
    "                related.append(th)\n",
    "        \n",
    "        a_dirige.sort(key=lambda x:x['year'])\n",
    "\n",
    "        # Update new persons to search:\n",
    "        co_directors = {d for th in a_dirige+[own_thesis, ]\n",
    "                          for d in th['directors'] if d!=full_id}\n",
    "        students = {th['author'] for th in a_dirige}\n",
    "\n",
    "        new_person = [p[1] for p in co_directors | students\n",
    "                      if is_valid_id(p[1]) and p[1] not in self.already_asked]\n",
    "\n",
    "        self.to_search.update(new_person)\n",
    "        \n",
    "        # Save thesis:\n",
    "        self.thesis[own_thesis['short_id']] = own_thesis\n",
    "        self.thesis.update({th['short_id']:th for th in a_dirige})\n",
    "        \n",
    "        # Build 'chain' (for subway graph)\n",
    "        if len(a_dirige) > 0:\n",
    "            chain = [own_thesis['short_id'], ] + [th['short_id'] for th in a_dirige]\n",
    "            self.chains.append(chain)\n",
    "            \n",
    "        \n",
    "    def save(self, name='blob.p'):\n",
    "        pickle.dump(self, open( name, \"wb\" ))\n",
    "        print(\"saved\")\n",
    "        \n",
    "    def print_info(self):\n",
    "        print('nbr thesis:', len(blob.thesis))\n",
    "        print('nbr to search:', len(blob.to_search))\n",
    "        print(self.seed, '+', self.nbr_gen, 'generations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seedlist loaded: 256 seeds\n",
      "http://www.theses.fr/028295730/id\n"
     ]
    }
   ],
   "source": [
    "# Get a random seed\n",
    "seedlist_file = \"data/seedlist.pick\"\n",
    "try:\n",
    "    seedlist = pickle.load( open( seedlist_file, \"rb\" ) )\n",
    "    print('seedlist loaded:', len(seedlist), \"seeds\")\n",
    "except FileNotFoundError:\n",
    "    seedlist = ['http://www.theses.fr/068670648/id', ]\n",
    "    pickle.dump(seedlist, open( seedlist_file, \"wb\" ))\n",
    "    seedlist = pickle.load( open( seedlist_file, \"rb\" ) )\n",
    "    print('seedlist loaded:', len(seedlist), \"seeds\")\n",
    "    \n",
    "seed_id = random.choice(seedlist)\n",
    "print(seed_id)\n",
    "\n",
    "# New graph\n",
    "blob = Blob(seed_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Jean-Claude Gatina (21 theses)                    "
     ]
    }
   ],
   "source": [
    "blob.grow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Étienne Labyt (2 theses)                                              \n",
      "\n",
      "nbr thesis: 129\n",
      "nbr to search: 124\n",
      "http://www.theses.fr/028295730/id + 31 generations\n"
     ]
    }
   ],
   "source": [
    "# Grow\n",
    "for _ in range(30):\n",
    "    blob.grow()\n",
    "\n",
    "print('\\n')\n",
    "blob.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/028295730_129\n",
      "13 chains\n"
     ]
    }
   ],
   "source": [
    "# Export the graph (for the layout)\n",
    "filename_prefix = 'data/'+blob.name+'_'+str(len(blob.thesis))\n",
    "print(filename_prefix)\n",
    "\n",
    "# Chains\n",
    "chs = sorted(blob.chains, key=len, reverse=True)\n",
    "print(len(chs), \"chains\")\n",
    "\n",
    "with open(filename_prefix + '_chains.json', 'w') as outfile:\n",
    "    json.dump(chs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 thesis\n"
     ]
    }
   ],
   "source": [
    "# Extract chain info and update\n",
    "chain_info = dict()\n",
    "for chain in blob.chains:\n",
    "    chain_info[chain[0]] = {'length':len(chain),\n",
    "                            'color':'red',\n",
    "                            'name':blob.thesis[chain[0]]['author'][0],\n",
    "                            'year':blob.thesis[chain[0]]['year']}\n",
    "    blob.thesis[chain[0]]['nbr_thesis'] = len(chain)\n",
    "    #blob.thesis[chain[0]]['color'] = red\n",
    "    \n",
    "# Export the info\n",
    "with open(filename_prefix + '_chaininfo.json', 'w') as outfile:\n",
    "    json.dump(chain_info, outfile)\n",
    "\n",
    "# Export the thesis info\n",
    "with open(filename_prefix + '_thesis.json', 'w') as outfile:\n",
    "    json.dump(blob.thesis, outfile)\n",
    "    \n",
    "print(len(blob.thesis), 'thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "#blobname = 'data/grosblob6.p'\n",
    "#blob.save(blobname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-projects",
   "language": "python",
   "name": "py3-projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
